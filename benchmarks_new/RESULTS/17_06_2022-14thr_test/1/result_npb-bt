

 NAS Parallel Benchmarks 3.0 structured OpenMP C version - BT Benchmark

 No input file inputbt.data. Using compiled defaults
 Size: 162x162x162
 Iterations: 200   dt:   0.000100
 Time step    1
 Time step   20
 Time step   40
 Time step   60
 Time step   80
 Time step  100
 Time step  120
 Time step  140
 Time step  160
 Time step  180
 Time step  200
 Verification being performed for class C
 accuracy setting for epsilon =  1.0000000000000e-08
 Comparison of RMS-norms of residual
           0 6.2398116551764e+03 6.2398116551765e+03 8.4538918188450e-15
           1 5.0793239190426e+02 5.0793239190424e+02 3.1223277182045e-14
           2 1.5423530093014e+03 1.5423530093014e+03 1.6953299613177e-14
           3 1.3302387929291e+03 1.3302387929291e+03 3.4869100222437e-14
           4 1.1604087428436e+04 1.1604087428436e+04 1.0032268590260e-14
 Comparison of RMS-norms of solution error
           0 1.6462008369091e+02 1.6462008369091e+02 2.7624050522310e-15
           1 1.1497107903824e+01 1.1497107903824e+01 5.4076633792685e-15
           2 4.1207446207461e+01 4.1207446207462e+01 1.7415497177492e-14
           3 3.7087651059694e+01 3.7087651059694e+01 4.4064486306179e-15
           4 3.6211053051843e+02 3.6211053051841e+02 3.8930565916706e-14
 Verification Successful


 BT Benchmark Completed
 Class           =                        C
 Size            =              162x162x162
 Iterations      =                      200
 Threads         =                       14
 Time in seconds =                   234.30
 Mop/s total     =                 12233.28
 Operation type  =           floating point
 Verification    =               SUCCESSFUL
 Version         =           3.0 structured
 Compile date    =              24 Aug 2021

 Compile options:
    CC           = icc
    CLINK        = icc 
    C_LIB        = (none)
    C_INC        = -I../common
    CFLAGS       = -O3 -openmp -lpthread -mcmodel=large
    CLINKFLAGS   = -openmp -lpthread -mcmodel=large
    RAND         = (none)
Elapsed Time: 236.364s
    Clockticks: 2,353,000,000
    Instructions Retired: 1,547,000,000
    CPI Rate: 1.521
     | The CPI may be too high. This could be caused by issues such as memory
     | stalls, instruction starvation, branch misprediction or long latency
     | instructions. Explore the other hardware-related metrics to identify what
     | is causing high CPI.
     |
    MUX Reliability: 0.700
    Retiring: 23.4% of Pipeline Slots
        Light Operations: 21.4% of Pipeline Slots
        Heavy Operations: 1.9% of Pipeline Slots
            Microcode Sequencer: 1.9% of Pipeline Slots
                Assists: 0.0% of Pipeline Slots
    Front-End Bound: 0.0% of Pipeline Slots
        Front-End Latency: 0.0% of Pipeline Slots
            ICache Misses: 0.0% of Clockticks
            ITLB Overhead: 0.0% of Clockticks
            Branch Resteers: 0.0% of Clockticks
            DSB Switches: 0.0% of Clockticks
            Length Changing Prefixes: 0.0% of Clockticks
            MS Switches: 0.0% of Clockticks
        Front-End Bandwidth: 0.0% of Pipeline Slots
            Front-End Bandwidth MITE: 0.0% of Clockticks
            Front-End Bandwidth DSB: 8.5% of Clockticks
            (Info) DSB Coverage: 90.9%
    Bad Speculation: 2.1% of Pipeline Slots
        Branch Mispredict: 0.0% of Pipeline Slots
        Machine Clears: 2.1% of Pipeline Slots
    Back-End Bound: 74.5% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 38.9% of Pipeline Slots
         | The metric value is high. This can indicate that the significant
         | fraction of execution pipeline slots could be stalled due to demand
         | memory load and stores. Use Memory Access analysis to have the metric
         | breakdown by memory hierarchy, memory bandwidth information,
         | correlation by memory objects.
         |
            L1 Bound: 8.9% of Clockticks
                DTLB Overhead: 0.0% of Clockticks
                Loads Blocked by Store Forwarding: 0.0% of Clockticks
                Lock Latency: 0.0% of Clockticks
                Split Loads: 0.0% of Clockticks
                4K Aliasing: 0.0% of Clockticks
                FB Full: 0.0% of Clockticks
            L2 Bound: N/A with HT on
            L3 Bound: N/A with HT on
                Contested Accesses: 0.0% of Clockticks
                Data Sharing: 0.0% of Clockticks
                L3 Latency: 0.0% of Clockticks
                SQ Full: 0.0% of Clockticks
            DRAM Bound: N/A with HT on
                Memory Bandwidth: N/A with HT on
                Memory Latency: N/A with HT on
                    Local DRAM: 0.0% of Clockticks
                    Remote DRAM: 0.0% of Clockticks
                    Remote Cache: 0.0% of Clockticks
            Store Bound: 98.2% of Clockticks
             | Issue: CPU was stalled on store operations for a significant
             | fraction of cycles.
             | 
             | Tips: Consider False Sharing analysis as your next step.
             |
                Store Latency: 8.0% of Clockticks
                False Sharing: 0.0% of Clockticks
                Split Stores: 0.0% of Clockticks
                DTLB Store Overhead: 0.0% of Clockticks
        Core Bound: 35.6% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
            Divider: 0.0% of Clockticks
            Port Utilization: 98.2% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Core
             | non-divider-related issues.
             | 
             | Tips: Use vectorization to reduce pressure on the execution ports
             | as multiple elements are calculated with same uOp.
             |
                Cycles of 0 Ports Utilized: 25.5% of Clockticks
                 | CPU executed no uOps on any execution port during a
                 | significant fraction of cycles. Long-latency instructions
                 | like divides may contribute to this issue. Check the Assembly
                 | view and Appendix C in the Optimization Guide to identify
                 | instructions with 5 or more cycles latency.
                 |
                Cycles of 1 Port Utilized: 8.5% of Clockticks
                Cycles of 2 Ports Utilized: 4.2% of Clockticks
                Cycles of 3+ Ports Utilized: 12.7% of Clockticks
                    ALU Operation Utilization: 2.1% of Clockticks
                        Port 0: 0.0% of Clockticks
                        Port 1: 0.0% of Clockticks
                        Port 5: 0.0% of Clockticks
                        Port 6: 8.5% of Clockticks
                    Load Operation Utilization: 0.0% of Clockticks
                        Port 2: 0.0% of Clockticks
                        Port 3: 8.5% of Clockticks
                    Store Operation Utilization: 8.5% of Clockticks
                        Port 4: 8.5% of Clockticks
                        Port 7: 0.0% of Clockticks
    Average CPU Frequency: 3.223 GHz 
    Total Thread Count: 14
    Paused Time: 0s
Effective Physical Core Utilization: 0.0% (0.003 out of 14)
 | The metric value is low, which may signal a poor physical CPU cores
 | utilization caused by:
 |     - load imbalance
 |     - threading runtime overhead
 |     - contended synchronization
 |     - thread/process underutilization
 |     - incorrect affinity that utilizes logical cores instead of physical
 |       cores
 | Explore sub-metrics to estimate the efficiency of MPI and OpenMP parallelism
 | or run the Locks and Waits analysis to identify parallel bottlenecks for
 | other parallel runtimes.
 |
    Effective Logical Core Utilization: 0.0% (0.003 out of 28)
     | The metric value is low, which may signal a poor logical CPU cores
     | utilization. Consider improving physical core utilization as the first
     | step and then look at opportunities to utilize logical cores, which in
     | some cases can improve processor throughput and overall performance of
     | multi-threaded applications.
     |
Collection and Platform Info
    Application Command Line: ./bin/omp_npb-bt_np_STD 
    User Name: vadim
    Operating System: 3.10.0-957.1.3.el7.x86_64 NAME="CentOS Linux" VERSION="7 (Core)" ID="centos" ID_LIKE="rhel fedora" VERSION_ID="7" PRETTY_NAME="CentOS Linux 7 (Core)" ANSI_COLOR="0;31" CPE_NAME="cpe:/o:centos:centos:7" HOME_URL="https://www.centos.org/" BUG_REPORT_URL="https://bugs.centos.org/"  CENTOS_MANTISBT_PROJECT="CentOS-7" CENTOS_MANTISBT_PROJECT_VERSION="7" REDHAT_SUPPORT_PRODUCT="centos" REDHAT_SUPPORT_PRODUCT_VERSION="7" 
    Computer Name: n50003.10p.parallel.ru
    Result Size: 6.0 MB 
    Collection start time: 18:38:09 17/06/2022 UTC
    Collection stop time: 18:42:05 17/06/2022 UTC
    Collector Type: Event-based sampling driver
    CPU
        Name: Intel(R) Xeon(R) E5/E7 v3 Processor code named Haswell
        Frequency: 2.600 GHz 
        Logical CPU Count: 28
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: not detected

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
