

 NAS Parallel Benchmarks 3.0 structured OpenMP C version - BT Benchmark

 No input file inputbt.data. Using compiled defaults
 Size: 162x162x162
 Iterations: 200   dt:   0.000100
 Time step    1
 Time step   20
 Time step   40
 Time step   60
 Time step   80
 Time step  100
 Time step  120
 Time step  140
 Time step  160
 Time step  180
 Time step  200
 Verification being performed for class C
 accuracy setting for epsilon =  1.0000000000000e-08
 Comparison of RMS-norms of residual
           0 6.2398116551764e+03 6.2398116551765e+03 8.4538918188450e-15
           1 5.0793239190426e+02 5.0793239190424e+02 3.1223277182045e-14
           2 1.5423530093014e+03 1.5423530093014e+03 1.6953299613177e-14
           3 1.3302387929291e+03 1.3302387929291e+03 3.4869100222437e-14
           4 1.1604087428436e+04 1.1604087428436e+04 1.0032268590260e-14
 Comparison of RMS-norms of solution error
           0 1.6462008369091e+02 1.6462008369091e+02 2.7624050522310e-15
           1 1.1497107903824e+01 1.1497107903824e+01 5.4076633792685e-15
           2 4.1207446207461e+01 4.1207446207462e+01 1.7415497177492e-14
           3 3.7087651059694e+01 3.7087651059694e+01 4.4064486306179e-15
           4 3.6211053051843e+02 3.6211053051841e+02 3.8930565916706e-14
 Verification Successful


 BT Benchmark Completed
 Class           =                        C
 Size            =              162x162x162
 Iterations      =                      200
 Threads         =                       14
 Time in seconds =                   234.35
 Mop/s total     =                 12230.93
 Operation type  =           floating point
 Verification    =               SUCCESSFUL
 Version         =           3.0 structured
 Compile date    =              24 Aug 2021

 Compile options:
    CC           = icc
    CLINK        = icc 
    C_LIB        = (none)
    C_INC        = -I../common
    CFLAGS       = -O3 -openmp -lpthread -mcmodel=large
    CLINKFLAGS   = -openmp -lpthread -mcmodel=large
    RAND         = (none)
Elapsed Time: 236.521s
    Clockticks: 10,193,352,000,000
    Instructions Retired: 6,712,888,000,000
    CPI Rate: 1.518
     | The CPI may be too high. This could be caused by issues such as memory
     | stalls, instruction starvation, branch misprediction or long latency
     | instructions. Explore the other hardware-related metrics to identify what
     | is causing high CPI.
     |
    MUX Reliability: 0.999
    Retiring: 16.6% of Pipeline Slots
        Light Operations: 16.1% of Pipeline Slots
        Heavy Operations: 0.4% of Pipeline Slots
            Microcode Sequencer: 0.4% of Pipeline Slots
                Assists: 0.0% of Pipeline Slots
    Front-End Bound: 1.5% of Pipeline Slots
        Front-End Latency: 0.6% of Pipeline Slots
            ICache Misses: 0.3% of Clockticks
            ITLB Overhead: 0.0% of Clockticks
            Branch Resteers: 0.1% of Clockticks
            DSB Switches: 0.2% of Clockticks
            Length Changing Prefixes: 0.0% of Clockticks
            MS Switches: 0.5% of Clockticks
        Front-End Bandwidth: 0.9% of Pipeline Slots
            Front-End Bandwidth MITE: 6.2% of Clockticks
            Front-End Bandwidth DSB: 2.2% of Clockticks
            (Info) DSB Coverage: 59.5%
    Bad Speculation: 0.1% of Pipeline Slots
        Branch Mispredict: 0.0% of Pipeline Slots
        Machine Clears: 0.1% of Pipeline Slots
    Back-End Bound: 81.8% of Pipeline Slots
     | A significant portion of pipeline slots are remaining empty. When
     | operations take too long in the back-end, they introduce bubbles in the
     | pipeline that ultimately cause fewer pipeline slots containing useful
     | work to be retired per cycle than the machine is capable to support. This
     | opportunity cost results in slower execution. Long-latency operations
     | like divides and memory operations can cause this, as can too many
     | operations being directed to a single execution port (for example, more
     | multiply operations arriving in the back-end per cycle than the execution
     | unit can support).
     |
        Memory Bound: 56.4% of Pipeline Slots
         | The metric value is high. This can indicate that the significant
         | fraction of execution pipeline slots could be stalled due to demand
         | memory load and stores. Use Memory Access analysis to have the metric
         | breakdown by memory hierarchy, memory bandwidth information,
         | correlation by memory objects.
         |
            L1 Bound: 3.7% of Clockticks
                DTLB Overhead: 1.1% of Clockticks
                Loads Blocked by Store Forwarding: 1.4% of Clockticks
                Lock Latency: 0.0% of Clockticks
                Split Loads: 0.0% of Clockticks
                4K Aliasing: 0.4% of Clockticks
                FB Full: 86.2% of Clockticks
            L2 Bound: N/A with HT on
            L3 Bound: N/A with HT on
                Contested Accesses: 0.0% of Clockticks
                Data Sharing: 0.0% of Clockticks
                L3 Latency: 32.9% of Clockticks
                 | This metric shows a fraction of cycles with demand load
                 | accesses that hit the L3 cache under unloaded scenarios
                 | (possibly L3 latency limited). Avoiding private cache misses
                 | (i.e. L2 misses/L3 hits) will improve the latency, reduce
                 | contention with sibling physical cores and increase
                 | performance. Note the value of this node may overlap with its
                 | siblings.
                 |
                SQ Full: 10.1% of Clockticks
                 | This metric measures fraction of cycles where the Super Queue
                 | (SQ) was full taking into account all request-types and both
                 | hardware SMT threads. The Super Queue is used for requests to
                 | access the L2 cache or to go out to the Uncore.
                 |
            DRAM Bound: N/A with HT on
                Memory Bandwidth: N/A with HT on
                Memory Latency: N/A with HT on
                    Local DRAM: 100.0% of Clockticks
                     | The number of CPU stalls on loads from the local memory
                     | exceeds the threshold. Consider caching data to improve
                     | the latency and increase the performance.
                     |
                    Remote DRAM: 0.0% of Clockticks
                    Remote Cache: 0.0% of Clockticks
            Store Bound: 35.1% of Clockticks
             | Issue: CPU was stalled on store operations for a significant
             | fraction of cycles.
             | 
             | Tips: Consider False Sharing analysis as your next step.
             |
                Store Latency: 70.4% of Clockticks
                 | This metric represents a fraction of cycles the CPU spent
                 | handling long-latency store misses (missing the 2nd level
                 | cache). Consider avoiding/reducing unnecessary (or easily
                 | loadable/computable) memory store. Note that this metric
                 | value may be highlighted due to a Lock Latency issue.
                 |
                False Sharing: 0.0% of Clockticks
                Split Stores: 0.0% of Clockticks
                DTLB Store Overhead: 0.6% of Clockticks
        Core Bound: 25.5% of Pipeline Slots
         | This metric represents how much Core non-memory issues were of a
         | bottleneck. Shortage in hardware compute resources, or dependencies
         | software's instructions are both categorized under Core Bound. Hence
         | it may indicate the machine ran out of an OOO resources, certain
         | execution units are overloaded or dependencies in program's data- or
         | instruction- flow are limiting the performance (e.g. FP-chained long-
         | latency arithmetic operations).
         |
            Divider: 1.5% of Clockticks
            Port Utilization: 34.2% of Clockticks
             | Issue: A significant fraction of cycles was stalled due to Core
             | non-divider-related issues.
             | 
             | Tips: Use vectorization to reduce pressure on the execution ports
             | as multiple elements are calculated with same uOp.
             |
                Cycles of 0 Ports Utilized: 14.4% of Clockticks
                Cycles of 1 Port Utilized: 3.5% of Clockticks
                Cycles of 2 Ports Utilized: 3.2% of Clockticks
                Cycles of 3+ Ports Utilized: 7.7% of Clockticks
                    ALU Operation Utilization: 10.1% of Clockticks
                        Port 0: 14.2% of Clockticks
                        Port 1: 15.0% of Clockticks
                        Port 5: 5.2% of Clockticks
                        Port 6: 6.1% of Clockticks
                    Load Operation Utilization: 12.6% of Clockticks
                        Port 2: 15.2% of Clockticks
                        Port 3: 15.5% of Clockticks
                    Store Operation Utilization: 9.1% of Clockticks
                        Port 4: 9.1% of Clockticks
                        Port 7: 3.6% of Clockticks
    Average CPU Frequency: 3.100 GHz 
    Total Thread Count: 14
    Paused Time: 0s
Effective Physical Core Utilization: 95.4% (13.362 out of 14)
    Effective Logical Core Utilization: 47.8% (13.379 out of 28)
     | The metric value is low, which may signal a poor utilization of logical
     | CPU cores while the utilization of physical cores is acceptable. Consider
     | using logical cores, which in some cases can improve processor throughput
     | and overall performance of multi-threaded applications.
     |
Collection and Platform Info
    Application Command Line: ./bin/omp_npb-bt_np_STD 
    User Name: vadim
    Operating System: 3.10.0-957.1.3.el7.x86_64 NAME="CentOS Linux" VERSION="7 (Core)" ID="centos" ID_LIKE="rhel fedora" VERSION_ID="7" PRETTY_NAME="CentOS Linux 7 (Core)" ANSI_COLOR="0;31" CPE_NAME="cpe:/o:centos:centos:7" HOME_URL="https://www.centos.org/" BUG_REPORT_URL="https://bugs.centos.org/"  CENTOS_MANTISBT_PROJECT="CentOS-7" CENTOS_MANTISBT_PROJECT_VERSION="7" REDHAT_SUPPORT_PRODUCT="centos" REDHAT_SUPPORT_PRODUCT_VERSION="7" 
    Computer Name: n50003.10p.parallel.ru
    Result Size: 642.1 MB 
    Collection start time: 18:57:21 17/06/2022 UTC
    Collection stop time: 19:01:17 17/06/2022 UTC
    Collector Type: Event-based sampling driver
    CPU
        Name: Intel(R) Xeon(R) E5/E7 v3 Processor code named Haswell
        Frequency: 2.600 GHz 
        Logical CPU Count: 28
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: not detected

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
