Elapsed Time: 0.068s
 | Application execution time is too short. Metrics data may be unreliable.
 | Consider reducing the sampling interval or increasing your application
 | execution time.
 |
    Clockticks: 2,574,000,000
    Instructions Retired: 2,548,000,000
    CPI Rate: 1.010
     | The CPI may be too high. This could be caused by issues such as memory
     | stalls, instruction starvation, branch misprediction or long latency
     | instructions. Explore the other hardware-related metrics to identify what
     | is causing high CPI.
     |
    MUX Reliability: 0.681
     | Precision of collected HW event data is not enough. Metrics data may be
     | unreliable. Consider increasing your application execution time, using
     | the multiple runs mode instead of event multiplexing, or creating a
     | custom analysis with a limited subset of HW events. If you are using a
     | driverless collection, consider reducing the value of
     | /sys/bus/event_source/devices/cpu/perf_event_mux_interval_ms file.
     |
    Retiring: 85.7% of Pipeline Slots
     | A high fraction of pipeline slots was utilized by useful work. While the
     | goal is to make this metric value as big as possible, a high Retiring
     | value for non-vectorized code could prompt you to consider code
     | vectorization. Vectorization enables doing more computations without
     | significantly increasing the number of instructions, thus improving the
     | performance. Note that this metric value may be highlighted due to
     | Microcode Sequencer (MS) issue, so the performance can be improved by
     | avoiding using the MS.
     |
        Light Operations: 85.7% of Pipeline Slots
         | CPU retired regular uops (ones which did not originate from the
         | microcode-sequencer) in a significant fraction of cycles. This
         | correlates with total number of instructions used by the program.
         | Optimum value of uops-per-instruction ratio is 1. While this is the
         | most desirable metric, high values can also provide opportunities for
         | performance optimizations.
         |
        Heavy Operations: 0.0% of Pipeline Slots
            Microcode Sequencer: 0.0% of Pipeline Slots
                Assists: 0.0% of Pipeline Slots
    Front-End Bound: 0.0% of Pipeline Slots
        Front-End Latency: 0.0% of Pipeline Slots
            ICache Misses: 0.0% of Clockticks
            ITLB Overhead: 0.0% of Clockticks
            Branch Resteers: 0.0% of Clockticks
            DSB Switches: 0.0% of Clockticks
            Length Changing Prefixes: 0.0% of Clockticks
            MS Switches: 0.0% of Clockticks
        Front-End Bandwidth: 0.0% of Pipeline Slots
            Front-End Bandwidth MITE: 0.0% of Clockticks
            Front-End Bandwidth DSB: 16.3% of Clockticks
            (Info) DSB Coverage: 0.0%
    Bad Speculation: 0.0% of Pipeline Slots
        Branch Mispredict: 0.0% of Pipeline Slots
        Machine Clears: 0.0% of Pipeline Slots
    Back-End Bound: 14.3% of Pipeline Slots
        Memory Bound: 7.5% of Pipeline Slots
            L1 Bound: 8.2% of Clockticks
                DTLB Overhead: 0.0% of Clockticks
                Loads Blocked by Store Forwarding: 0.0% of Clockticks
                Lock Latency: 0.0% of Clockticks
                Split Loads: 0.0% of Clockticks
                4K Aliasing: 0.0% of Clockticks
                FB Full: 0.0% of Clockticks
            L2 Bound: N/A with HT on
            L3 Bound: N/A with HT on
                Contested Accesses: 0.0% of Clockticks
                Data Sharing: 0.0% of Clockticks
                L3 Latency: 0.0% of Clockticks
                SQ Full: 0.0% of Clockticks
            DRAM Bound: N/A with HT on
                Memory Bandwidth: N/A with HT on
                Memory Latency: N/A with HT on
                    Local DRAM: 0.0% of Clockticks
                    Remote DRAM: 0.0% of Clockticks
                    Remote Cache: 0.0% of Clockticks
            Store Bound: 40.8% of Clockticks
                Store Latency: 100.0% of Clockticks
                False Sharing: 0.0% of Clockticks
                Split Stores: 0.0% of Clockticks
                DTLB Store Overhead: 0.0% of Clockticks
        Core Bound: 6.9% of Pipeline Slots
            Divider: 0.0% of Clockticks
            Port Utilization: 44.9% of Clockticks
                Cycles of 0 Ports Utilized: 100.0% of Clockticks
                Cycles of 1 Port Utilized: 24.5% of Clockticks
                Cycles of 2 Ports Utilized: 32.6% of Clockticks
                Cycles of 3+ Ports Utilized: 65.3% of Clockticks
                    ALU Operation Utilization: 57.1% of Clockticks
                        Port 0: 49.0% of Clockticks
                        Port 1: 65.3% of Clockticks
                        Port 5: 49.0% of Clockticks
                        Port 6: 65.3% of Clockticks
                    Load Operation Utilization: 8.2% of Clockticks
                        Port 2: 16.3% of Clockticks
                        Port 3: 32.6% of Clockticks
                    Store Operation Utilization: 32.6% of Clockticks
                        Port 4: 32.6% of Clockticks
                        Port 7: 0.0% of Clockticks
    Average CPU Frequency: 3.101 GHz 
    Total Thread Count: 14
    Paused Time: 0s
Effective Physical Core Utilization: 43.8% (6.130 out of 14)
 | The metric value is low, which may signal a poor physical CPU cores
 | utilization caused by:
 |     - load imbalance
 |     - threading runtime overhead
 |     - contended synchronization
 |     - thread/process underutilization
 |     - incorrect affinity that utilizes logical cores instead of physical
 |       cores
 | Explore sub-metrics to estimate the efficiency of MPI and OpenMP parallelism
 | or run the Locks and Waits analysis to identify parallel bottlenecks for
 | other parallel runtimes.
 |
    Effective Logical Core Utilization: 43.8% (12.259 out of 28)
     | The metric value is low, which may signal a poor logical CPU cores
     | utilization. Consider improving physical core utilization as the first
     | step and then look at opportunities to utilize logical cores, which in
     | some cases can improve processor throughput and overall performance of
     | multi-threaded applications.
     |
Collection and Platform Info
    Application Command Line: ./bin/omp_triada_np_STD 
    User Name: vadim
    Operating System: 3.10.0-957.1.3.el7.x86_64 NAME="CentOS Linux" VERSION="7 (Core)" ID="centos" ID_LIKE="rhel fedora" VERSION_ID="7" PRETTY_NAME="CentOS Linux 7 (Core)" ANSI_COLOR="0;31" CPE_NAME="cpe:/o:centos:centos:7" HOME_URL="https://www.centos.org/" BUG_REPORT_URL="https://bugs.centos.org/"  CENTOS_MANTISBT_PROJECT="CentOS-7" CENTOS_MANTISBT_PROJECT_VERSION="7" REDHAT_SUPPORT_PRODUCT="centos" REDHAT_SUPPORT_PRODUCT_VERSION="7" 
    Computer Name: n50003.10p.parallel.ru
    Result Size: 5.3 MB 
    Collection start time: 10:46:54 17/06/2022 UTC
    Collection stop time: 10:49:41 17/06/2022 UTC
    Collector Type: Event-based sampling driver
    CPU
        Name: Intel(R) Xeon(R) E5/E7 v3 Processor code named Haswell
        Frequency: 2.600 GHz 
        Logical CPU Count: 28
        Cache Allocation Technology
            Level 2 capability: not detected
            Level 3 capability: not detected

If you want to skip descriptions of detected performance issues in the report,
enter: vtune -report summary -report-knob show-issues=false -r <my_result_dir>.
Alternatively, you may view the report in the csv format: vtune -report
<report_name> -format=csv.
